{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e070ccb6-cf8d-42be-882d-c16117bad638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/517399182210612', creation_time=1768928607309, experiment_id='517399182210612', last_update_time=1769010275293, lifecycle_stage='active', name='/Users/keerthi.amulya.1999@gmail.com/Day-12', tags={'mlflow.experiment.sourceName': '/Users/keerthi.amulya.1999@gmail.com/Day-12',\n",
       " 'mlflow.experimentType': 'NOTEBOOK',\n",
       " 'mlflow.ownerEmail': 'keerthi.amulya.1999@gmail.com',\n",
       " 'mlflow.ownerId': '73807756678194'}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(\"/Workspace/Users/keerthi.amulya.1999@gmail.com/Day-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9baa3c58-22a3-46b1-92fe-a6c58a42012c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- product_id: integer (nullable = true)\n |-- brand: string (nullable = true)\n |-- views: long (nullable = true)\n |-- purchases: long (nullable = true)\n |-- revenue: double (nullable = true)\n |-- conversion_rate: double (nullable = true)\n\n+----------+-------+-----+---------+------------------+-------------------+\n|product_id|  brand|views|purchases|           revenue|    conversion_rate|\n+----------+-------+-----+---------+------------------+-------------------+\n|   8500290|   NULL|  357|       12|           4071.73|  3.361344537815126|\n|   3300488|redmond| 1718|       38| 6847.049999999998|  2.211874272409779|\n|  12704683| nokian|  733|       29|3121.4700000000003|  3.956343792633015|\n|   5100799| garmin| 2450|        5|22594.309999999998|0.20408163265306123|\n|   1004573|samsung| 3216|       50|39517.170000000006|  1.554726368159204|\n+----------+-------+-----+---------+------------------+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.table(\"ecommerce.gold.products\")\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141d8f3b-067e-4b8d-abbd-ff48f01050db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pdf = df_spark.select(\"views\", \"purchases\", \"revenue\", \"conversion_rate\").toPandas()\n",
    "pdf = pdf.fillna(0)\n",
    "X = pdf[[\"views\", \"purchases\", \"conversion_rate\"]]\n",
    "y = pdf[\"revenue\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ae3544-4e86-4f51-ba7d-720c510fcba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:57:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear: R2=0.6405, RMSE=43655.59\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:57:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree: R2=0.9139, RMSE=21362.42\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:58:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest: R2=0.8177, RMSE=31091.45\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "models = {\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"decision_tree\": DecisionTreeRegressor(max_depth=6, random_state=42),\n",
    "    \"random_forest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{name}_model\"):\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"features\", \"views,purchases,conversion_rate\")\n",
    "        mlflow.log_param(\"target\", \"revenue\")\n",
    "\n",
    "        # log hyperparams if available\n",
    "        if hasattr(model, \"get_params\"):\n",
    "            for k, v in model.get_params().items():\n",
    "                if k in [\"max_depth\", \"n_estimators\", \"min_samples_split\", \"min_samples_leaf\"]:\n",
    "                    mlflow.log_param(k, v)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "        mlflow.log_metric(\"r2\", float(r2))\n",
    "        mlflow.log_metric(\"rmse\", float(rmse))\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        results.append((name, r2, rmse))\n",
    "        print(f\"{name}: R2={r2:.4f}, RMSE={rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d255f37-707f-4239-bd24-05204e7d2179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:58:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF cfg={'n_estimators': 200, 'max_depth': 6} -> R2=0.8281, RMSE=30193.45\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:58:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF cfg={'n_estimators': 300, 'max_depth': 10} -> R2=0.8205, RMSE=30852.32\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/22 02:59:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF cfg={'n_estimators': 500, 'max_depth': None} -> R2=0.8242, RMSE=30532.22\n"
     ]
    }
   ],
   "source": [
    "rf_grid = [\n",
    "    {\"n_estimators\": 200, \"max_depth\": 6},\n",
    "    {\"n_estimators\": 300, \"max_depth\": 10},\n",
    "    {\"n_estimators\": 500, \"max_depth\": None},\n",
    "]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for cfg in rf_grid:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=cfg[\"n_estimators\"],\n",
    "        max_depth=cfg[\"max_depth\"],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"rf_tune_{cfg['n_estimators']}_{cfg['max_depth']}\"):\n",
    "        mlflow.log_param(\"model_type\", \"random_forest\")\n",
    "        mlflow.log_param(\"features\", \"views,purchases,conversion_rate\")\n",
    "        mlflow.log_param(\"target\", \"revenue\")\n",
    "        mlflow.log_param(\"n_estimators\", cfg[\"n_estimators\"])\n",
    "        mlflow.log_param(\"max_depth\", str(cfg[\"max_depth\"]))\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "        mlflow.log_metric(\"r2\", float(r2))\n",
    "        mlflow.log_metric(\"rmse\", float(rmse))\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"RF cfg={cfg} -> R2={r2:.4f}, RMSE={rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ccbab1-e9c9-4be2-a8a7-81967d4b1a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>views</td>\n",
       "      <td>0.656576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchases</td>\n",
       "      <td>0.225020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conversion_rate</td>\n",
       "      <td>0.118404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "0            views    0.656576\n",
       "1        purchases    0.225020\n",
       "2  conversion_rate    0.118404"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_rf = models[\"random_forest\"]  \n",
    "fi = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": best_rf.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0317fb-15bf-4961-94f1-8a676fe68c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|revenue|         prediction|\n+-------+-------------------+\n|  12.15|-3541.2404146436525|\n|  17.37|-3541.2404146436525|\n|  27.03|-3541.2404146436525|\n| 116.06|-3541.2404146436525|\n| 157.53|-3541.2404146436525|\n+-------+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression as SparkLR\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark_df = spark.table(\"ecommerce.gold.products\") \\\n",
    "    .select(\"views\",\"purchases\",\"conversion_rate\",\"revenue\") \\\n",
    "    .na.fill(0)\n",
    "\n",
    "train_s, test_s = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"views\",\"purchases\",\"conversion_rate\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "lr = SparkLR(featuresCol=\"features\", labelCol=\"revenue\")\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "spark_model = pipeline.fit(train_s)\n",
    "pred_s = spark_model.transform(test_s)\n",
    "pred_s.select(\"revenue\",\"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73819356-4ad3-4b2b-a790-548a34b56e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7606715851453805>, line 5\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mviews,purchases,conversion_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevenue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 5\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mfloat\u001B[39m(r2_s))\n",
       "\u001B[1;32m      6\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mfloat\u001B[39m(rmse_s))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'r2_s' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'r2_s' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'r2_s' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN00",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-7606715851453805>, line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mviews,purchases,conversion_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevenue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mfloat\u001B[39m(r2_s))\n\u001B[1;32m      6\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mfloat\u001B[39m(rmse_s))\n",
        "\u001B[0;31mNameError\u001B[0m: name 'r2_s' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"spark_lr_pipeline\"):\n",
    "    mlflow.log_param(\"model_type\", \"spark_lr_pipeline\")\n",
    "    mlflow.log_param(\"features\", \"views,purchases,conversion_rate\")\n",
    "    mlflow.log_param(\"target\", \"revenue\")\n",
    "    mlflow.log_metric(\"r2\", float(r2_s))\n",
    "    mlflow.log_metric(\"rmse\", float(rmse_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65b3d6c5-c3b0-427b-bc21-ad355c51c69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day-13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}